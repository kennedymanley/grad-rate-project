time_trees = tribble(
~trees100, ~trees200,~trees300,~trees400,~trees500,
"numtrees", 100, 200, 300, 400, 500,
"time", time_100, time_200, time_300, time_400, time_500
)
time_trees = tribble(
~label, ~trees100, ~trees200,~trees300,~trees400,~trees500,
"numtrees", 100, 200, 300, 400, 500,
"time", time_100, time_200, time_300, time_400, time_500
)
time_trees[1]
time_trees[1,]
time_trees[,1]
time_trees = tribble(
~numtrees, ~time,
100, time_100,
200, time_200,
300, time_300,
400, time_400,
500, time_500,
)
time_trees
time_trees %>% ggplot(aes(x = "numtrees", y = "time")) +
labs(x = "Trees",
y = "Elapsed Time") +
theme_bw()
time_trees %>% ggplot(aes(x = numtrees, y = time)) +
labs(x = "Trees",
y = "Elapsed Time") +
theme_bw()
time_400
time_trees %>% ggplot(aes(x = numtrees, y = time)) +
labs(x = "Trees",
y = "Elapsed Time") +
theme_bw()
time_trees %>% ggplot(aes(x = numtrees, y = time)) +
labs(x = "Trees",
y = "Elapsed Time") +
geom_point() +
theme_bw()
spam_train
system.time(randomForest(spam ~ ., ntree = 500, m = 1, importance = FALSE, data = spam_train))
system.time(randomForest(spam ~ ., ntree = 500, m = 20, importance = FALSE, data = spam_train))
system.time(randomForest(spam ~ ., ntree = 500, mtry = 20, importance = FALSE, data = spam_train))
system.time(randomForest(spam ~ ., ntree = 100, mtry = 20, importance = FALSE, data = spam_train))
rf_3 = randomForest(spam ~ ., ntree = 100, mtry = 3, importance = FALSE, data = spam_train)
rf_12 = randomForest(spam ~ ., ntree = 100, mtry = 12, importance = FALSE, data = spam_train)
rf_22 = randomForest(spam ~ ., ntree = 100, mtry = 22, importance = FALSE, data = spam_train)
rf_38 = randomForest(spam ~ ., ntree = 100, mtry = 38, importance = FALSE, data = spam_train)
rf_44 = randomForest(spam ~ ., ntree = 100, mtry = 44, importance = FALSE, data = spam_train)
oob_errors = bind_rows(
tibble(ntree = 1:500, oob_err = rf_3$mse, m = 3),
tibble(ntree = 1:500, oob_err = rf_12$mse, m = 12),
tibble(ntree = 1:500, oob_err = rf_22$mse, m = 22),
tibble(ntree = 1:500, oob_err = rf_38$mse, m = 38),
tibble(ntree = 1:500, oob_err = rf_44$mse, m = 44)
)
mvalues = seq(1,58, by = 11)
oob_errors = numeric(length(mvalues))
ntree = 100
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit = randomForest(spam ~ ., importance = FALSE, mtry = m, data = spam_train)
oob_errors[idx] = rf_fit$mse[ntree]
}
tibble(m = mvalues, oob_err = oob_errors) %>%
ggplot(aes(x = m, y = oob_err)) +
geom_line() + geom_point() +
scale_x_continuous(breaks = mvalues) +
theme_bw()
rf_12 = randomForest(spam ~ ., ntree = 500, mtry = 12, importance = TRUE, data = spam_train)
oob_errors = tibble(ntree = 1:500, oob_err = rf_12$mse, m = 12)
oob_errors %>%
ggplot(aes(x = ntree, y = oob_err)) +
geom_line() + theme_bw()
spam_train
spam_train %>% select(spam)
## Variable importance (6 points)
rf_fit = randomForest(factor(spam) ~ ., mtry = 12, importance = TRUE, data = spam_train)
varImpPlot(rf_fit)
varImpPlot(rf_fit, n.var = 10)
set.seed(1) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 1
gbm_fit1 = gbm(spam~ .,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = 1,
shrinkage = 0.1,
cv.folds = 5,
data = spam_train)
gbm_fit2 = gbm(spam~ .,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = 2,
shrinkage = 0.1,
cv.folds = 5,
data = spam_train)
set.seed(1) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 3
gbm_fit3 = gbm(spam~ .,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = 3,
shrinkage = 0.1,
cv.folds = 5,
data = spam_train)
opt_num_trees = gbm.perf(gbm_fit1)
opt_num_trees
opt_num_trees2 = gbm.perf(gbm_fit2)
opt_num_trees2
opt_num_trees3 = gbm.perf(gbm_fit3)
plot_grid(opt_num_trees1, opt_num_trees2, opt_num_trees3, align = "h")
library(cowplot)
plot_grid(opt_num_trees1, opt_num_trees2, opt_num_trees3, align = "h")
opt_num_trees1 = gbm.perf(gbm_fit1)
opt_num_trees2 = gbm.perf(gbm_fit2)
opt_num_trees3 = gbm.perf(gbm_fit3)
plot_grid(opt_num_trees1, opt_num_trees2, opt_num_trees3, align = "h")
opt_num_trees1 = gbm.perf(gbm_fit1)
opt_num_trees2 = gbm.perf(gbm_fit2)
opt_num_trees3 = gbm.perf(gbm_fit3)
ntrees = 100
cv_errors = bind_rows(
tibble(ntree = 1:ntrees, cv_err = gbm_fit1$cv.error, depth = 1),
tibble(ntree = 1:ntrees, cv_err = gbm_fit2$cv.error, depth = 2),
tibble(ntree = 1:ntrees, cv_err = gbm_fit3$cv.error, depth = 3)
)
ntrees = 1000
cv_errors = bind_rows(
tibble(ntree = 1:ntrees, cv_err = gbm_fit1$cv.error, depth = 1),
tibble(ntree = 1:ntrees, cv_err = gbm_fit2$cv.error, depth = 2),
tibble(ntree = 1:ntrees, cv_err = gbm_fit3$cv.error, depth = 3)
)
cv_errors
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() +
geom_hline(min(cv_errors))
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() +
geom_hline(min(cv_errors))+
theme_bw()
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() +
theme_bw()
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth), hline = min(cv_errors))) +
geom_line() +
theme_bw()
gbm_fit_optimal = gbm_fit3
optimal_num_trees = gbm.perf(gbm_fit3, plot.it = FALSE)
optimal_num_trees
summary(gbm_fit_optimal, n.trees = optimal_num_trees, plotit = FALSE)
rif_table = summary(gbm_fit_optimal, n.trees = optimal_num_trees, plotit = FALSE) %>%
head(10)
rif_table %>% kable(format = "latex", row.names = NA,
booktabs = TRUE,
digits = 5,
col.names = c("Variable",
"Variable",
"Relative Influence"),
caption = "These are the first ten rows of the optimal boosting model.")%>%
kable_styling(position = "center") %>%
kable_styling(latex_options = "HOLD_position")
rif_table %>% kable(format = "latex", row.names = NA,
booktabs = TRUE,
digits = 5,
col.names = c("Variable",
"Relative Influence"),
caption = "These are the first ten rows of the optimal boosting model.")%>%
kable_styling(position = "center") %>%
kable_styling(latex_options = "HOLD_position")
summary(gbm_fit_optimal, n.trees = optimal_num_trees, plotit = FALSE) %>%
head(10)
p1 = plot(gbm_fit_optimal, i.var = "char_freq_exclamation_mark", n.trees = optimal_num_trees)
p2 = plot(gbm_fit_optimal, i.var = "char_freq_dollar_sign", n.trees = optimal_num_trees)
p3 = plot(gbm_fit_optimal, i.var = "word_freq_remove", n.trees = optimal_num_trees)
plot_grid(p1, p2, p3, align = "h")
set.seed(1) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 1
gbm_fit1 = gbm(spam~ .,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 1,
shrinkage = 0.1,
cv.folds = 5,
data = spam_train)
set.seed(1) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 2
gbm_fit2 = gbm(spam~ .,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 2,
shrinkage = 0.1,
cv.folds = 5,
data = spam_train)
set.seed(1) # for reproducibility (DO NOT CHANGE)
# TODO: Fit random forest with interaction depth 3
gbm_fit3 = gbm(spam~ .,
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 3,
shrinkage = 0.1,
cv.folds = 5,
data = spam_train)
ntrees = 1000
cv_errors = bind_rows(
tibble(ntree = 1:ntrees, cv_err = gbm_fit1$cv.error, depth = 1),
tibble(ntree = 1:ntrees, cv_err = gbm_fit2$cv.error, depth = 2),
tibble(ntree = 1:ntrees, cv_err = gbm_fit3$cv.error, depth = 3)
)
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth), hline = min(cv_errors))) +
geom_line() +
theme_bw()
gbm_fit_optimal = gbm_fit3
optimal_num_trees = gbm.perf(gbm_fit3, plot.it = FALSE)
optimal_num_trees
rif_table = summary(gbm_fit_optimal, n.trees = optimal_num_trees, plotit = FALSE) %>%
head(10)
rif_table
p1 = plot(gbm_fit_optimal, i.var = "char_freq_exclamation_mark", n.trees = optimal_num_trees)
p2 = plot(gbm_fit_optimal, i.var = "char_freq_dollar_sign", n.trees = optimal_num_trees)
p3 = plot(gbm_fit_optimal, i.var = "word_freq_remove", n.trees = optimal_num_trees)
plot_grid(p1, p2, p3, align = "h")
gbm_probabilities = predict(gbm_fit_optimal, n.trees = optimal_num_trees,
type = "response", newdata = spam_test)
gbm_probabilities = predict(gbm_fit_optimal, n.trees = optimal_num_trees,
type = "response", newdata = spam_test)
gbm_predictions = as.numeric(gbm_probabilities > 0.5)
mce_bc = mean(gbm_predictions != spam_test$spam)
pred = predict(tree_fit, newdata = spam_test, type = "class")
pred = predict(tree_fit, newdata = spam_test, type = "class")
tree_fit = rpart(spam ~ .,
method = "class",              # classification
parms = list(split = "gini"),  # Gini index for splitting
data = spam_train)
pred = predict(tree_fit, newdata = spam_test, type = "class")
mce_tdt = mean(pred != spam_test$spam)
mce_bc
mce_tdt
mce_bc
pred_rf = predict(rf_fit, newdata = spam_test, type = "class")
mce_rf = mean(pred_rf != spam_test$spam)
mce_rf
misclassification_errors = tribble(
~Tuned_Decision_Trees, ~Random_Forest,~Boosting_Classifiers,
mce_tdt, mce_rf, mce_bc
)
misclassification_errors
rf_test_err = apply(
t(apply(
predict(rf_fit_tuned,
newdata = spam_test,
type = "response",
predict.all = TRUE)$individual,
1,
function(row)(as.numeric(cummean(as.numeric(row)) > 0.5))
)),
2,
function(pred)(mean(pred != spam_test$spam))
)
rf_test_err = apply(
t(apply(
predict(rf_fit,
newdata = spam_test,
type = "response",
predict.all = TRUE)$individual,
1,
function(row)(as.numeric(cummean(as.numeric(row)) > 0.5))
)),
2,
function(pred)(mean(pred != spam_test$spam))
)
rf_fit_tuned = rf_fit
gbm_fit_tuned = gbm_fit_optimal
gbm_test_err = apply(
predict(gbm_fit_tuned,
newdata = spam_test,
type = "response",
n.trees = 1:500),
2,
function(p)(mean(as.numeric(p > 0.5) != spam_test$spam))
)
rf_test_error %>%
ggplot(aes(x = ntree, y = oob_err)) +
geom_line() + theme_bw()
rf_test_err
rf_test_err %>%
ggplot(aes(x = ntree, y = err)) +
geom_line() + theme_bw()
rf_errors = tibble(ntree = 1:500, rf_error = rf_test_err)
rf_errors%>%
ggplot(aes(x = ntree, y = rf_err)) +
geom_line() + theme_bw()
rf_errors = tibble(ntree = 1:500, rf_error = rf_test_err)
rf_errors%>%
ggplot(aes(x = ntree, y = rf_err)) +
geom_line() + theme_bw()
rf_errors%>%
ggplot(aes(x = ntree, y = rf_error)) +
geom_line() + theme_bw()
rf_errors%>%
ggplot(aes(x = ntree, y = rf_error)) +
scale_y_log10()+
geom_line() + theme_bw()
rf_errors%>%
ggplot(aes(x = ntree, y = rf_error)) +
scale_y_log10()+
xlab("Number of Trees")+
ylab("Random Forest Error")+
geom_line() + theme_bw()
gbm_errors = tibble(ntree = 1:500, gbm_error = gbm_test_err)
p2 = rf_errors%>%
ggplot(aes(x = ntree, y = gbm_error)) +
scale_y_log10()+
xlab("Number of Trees")+
ylab("Boosting Classifiers Error")+
geom_line() + theme_bw()
plot_grid(p1, p2, align = "h")
gbm_errors = tibble(ntree = 1:500, gbm_error = gbm_test_err)
p2 = rf_errors%>%
ggplot(aes(x = ntree, y = gbm_error)) +
scale_y_log10()+
xlab("Number of Trees")+
ylab("Boosting Classifiers Error")+
geom_line() + theme_bw()
plot_grid(p1, p2, align = "h")
gbm_test_err = apply(
predict(gbm_fit_tuned,
newdata = spam_test,
type = "response",
n.trees = 1:500),
2,
function(p)(mean(as.numeric(p > 0.5) != spam_test$spam))
)
gbm_test_err = apply(
predict(gbm_fit_tuned,
newdata = spam_test,
type = "response",
n.trees = 1:500),
2,
function(p)(mean(as.numeric(p > 0.5) != spam_test$spam))
)
gbm_errors = tibble(ntree = 1:500, gbm_error = gbm_test_err)
p2 = rf_errors%>%
ggplot(aes(x = ntree, y = gbm_error)) +
scale_y_log10()+
xlab("Number of Trees")+
ylab("Boosting Classifiers Error")+
geom_line() + theme_bw()
plot_grid(p1, p2, align = "h")
rf_errors%>%
ggplot(aes(x = ntree, y = gbm_error)) +
scale_y_log10()+
xlab("Number of Trees")+
ylab("Boosting Classifiers Error")+
geom_line() + theme_bw()
p2 = gbm_errors%>%
ggplot(aes(x = ntree, y = gbm_error)) +
scale_y_log10()+
xlab("Number of Trees")+
ylab("Boosting Classifiers Error")+
geom_line() + theme_bw()
plot_grid(p1, p2, align = "h")
p1 = rf_errors%>%
ggplot(aes(x = ntree, y = rf_error)) +
scale_y_log10()+
xlab("Number of Trees")+
ylab("Random Forest Error")+
geom_line() + theme_bw()
gbm_errors = tibble(ntree = 1:500, gbm_error = gbm_test_err)
p2 = gbm_errors%>%
ggplot(aes(x = ntree, y = gbm_error)) +
scale_y_log10()+
xlab("Number of Trees")+
ylab("Boosting Classifiers Error")+
geom_line() + theme_bw()
plot_grid(p1, p2, align = "h")
spam_train
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth), hline = min(cv_errors))) +
geom_line() +
theme_bw()
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() +
theme_bw()
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() +
geom_hline(min(cv_errors))
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() +
geom_hline(min(cv_errors))+
theme_bw()
min(cv_errors)
cv_errors
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() +
geom_hline(min(factor(depth)))+
theme_bw()
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() +
theme_bw()
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() +
xlab("number of trees")+
ylab("cv error") +
theme_bw()
min1 = min(gbm_fit1)
min1 =gbm_fit1 %>% summarize(min(cv.error))
min1 =gbm_fit1 %>% summarize(min = min(cv.error))
min1 =cv_errors %>% group_by(depth) %>% summarize(min = min(cv.error))
cv_errors
min1 =cv_errors %>% group_by(depth) %>% summarize(min = min(cv_err))
min1
min1 =cv_errors %>% select(depth == 1) %>% summarize(min = min(cv_err))
min1 =cv_errors %>% select(depth = 1) %>% summarize(min = min(cv_err))
cv_errors %>% select(depth = 1)
min1 =cv_errors %>% group_by(depth) %>% summarize(min = min(cv_err))
min1
min =cv_errors %>% group_by(depth) %>% summarize(min = min(cv_err))
min
min1 = min %>%
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() +
xlab("number of trees")+
ylab("cv error") +
geom_hline(aes(yintercept = min), color = "red", linetype = "dashed")+
theme_bw()
min
min1 = min %>%
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() +
xlab("number of trees")+
ylab("cv error") +
geom_hline(aes(yintercept = 0.332), color = "red", linetype = "dashed")+
theme_bw()
cv_errors = bind_rows(
tibble(ntree = 1:ntrees, cv_err = gbm_fit1$cv.error, depth = 1),
tibble(ntree = 1:ntrees, cv_err = gbm_fit2$cv.error, depth = 2),
tibble(ntree = 1:ntrees, cv_err = gbm_fit3$cv.error, depth = 3)
)
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() +
xlab("number of trees")+
ylab("cv error") +
geom_hline(aes(yintercept = 0.332), color = "red", linetype = "dashed")+
theme_bw()
min
cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
geom_line() +
xlab("number of trees")+
ylab("cv error") +
geom_hline(aes(yintercept = 0.332), color = "red", linetype = "dashed")+
geom_hline(aes(yintercept = 0.314), color = "green", linetype = "dashed")+
geom_hline(aes(yintercept = 0.305), color = "blue", linetype = "dashed")+
theme_bw()
spam_train
tree_fit
rpart.plot(tree_fit)
library(rpart)         # to train decision trees
library(rpart.plot)
rpart.plot(tree_fit) # plot tree
#fit deepest possible tree
set.seed(1) # for reproducibility (DO NOT CHANGE)
save(tree_fit, file = "results/tree_fit.Rda")
save(tree_fit, file = "results/tree_fit.Rda")
load("~/Desktop/Stat471/stat-471-fall-2021/stat-471-fall-2021/grad-rate-project/results/ridge_fit.Rda")
# fit classification tree
tree_fit = rpart(grad_rate ~ . ,
method = "class", # classification
parms = list(split = "gini"), # Gini index for splitting
data = grad_train_clean)
library(rpart)         # to train decision trees
library(rpart.plot)    # to plot decision trees
library(randomForest)  # random forests
library(gbm)           # boosting
library(tidyverse)     # tidyverse
library(kableExtra)    # for printing tables
library(cowplot)       # for side by side plots
grad_train = read_csv("data/clean/grad_train.csv")
grad_train = grad_train %>% na.omit(grad_train)
grad_train_clean = grad_train %>% select(-report_school_year, -aggregation_index, -aggregation_name,
-county_code, -county_name, -membership_code, -membership_key,
-membership_desc, -subgroup_code, -subgroup_name)
# read in the training data
grad_train = read_csv("data/clean/grad_train.csv")
setwd("~/Desktop/Stat471/stat-471-fall-2021/stat-471-fall-2021/grad-rate-project")
# read in the training data
grad_train = read_csv("data/clean/grad_train.csv")
rpart.plot(tree_fit) # plot tree
grad_train = grad_train %>% na.omit(grad_train)
grad_train_clean = grad_train %>% select(-report_school_year, -aggregation_index, -aggregation_name,
-county_code, -county_name, -membership_code, -membership_key,
-membership_desc, -subgroup_code, -subgroup_name)
# fit classification tree
tree_fit = rpart(grad_rate ~ . ,
method = "class", # classification
parms = list(split = "gini"), # Gini index for splitting
data = grad_train_clean)
